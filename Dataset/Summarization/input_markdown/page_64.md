Conceptual guide | ü¶úÔ∏èüîó LangChain[Skip to main content](#__docusaurus_skipToContent_fallback)[![](/img/brand/wordmark.png)![](/img/brand/wordmark-dark.png)](/)[Integrations](/docs/integrations/providers/)[API Reference](https://python.langchain.com/api_reference/)[More](#)

* [Contributing](/docs/contributing/)
* [People](/docs/people/)
* [Error reference](/docs/troubleshooting/errors/)
* ---
* [LangSmith](https://docs.smith.langchain.com)
* [LangGraph](https://langchain-ai.github.io/langgraph/)
* [LangChain Hub](https://smith.langchain.com/hub)
* [LangChain JS/TS](https://js.langchain.com)
[v0.3](#)

* [v0.3](/docs/introduction/)
* [v0.2](https://python.langchain.com/v0.2/docs/introduction)
* [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)
[üí¨](https://chat.langchain.com)Search

* [Introduction](/docs/introduction/)
* [Tutorials](/docs/tutorials/)
  + [Build a Question Answering application over a Graph Database](/docs/tutorials/graph/)
  + [Tutorials](/docs/tutorials/)
  + [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm_chain/)
  + [Build a Chatbot](/docs/tutorials/chatbot/)
  + [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa_chat_history/)
  + [Build an Extraction Chain](/docs/tutorials/extraction/)
  + [Build an Agent](/docs/tutorials/agents/)
  + [Tagging](/docs/tutorials/classification/)
  + [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag/)
  + [Build a semantic search engine](/docs/tutorials/retrievers/)
  + [Build a Question/Answering system over SQL data](/docs/tutorials/sql_qa/)
  + [Summarize Text](/docs/tutorials/summarization/)
* [How-to guides](/docs/how_to/)
  + [How-to guides](/docs/how_to/)
  + [How to use tools in a chain](/docs/how_to/tools_chain/)
  + [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)
  + [How to add memory to chatbots](/docs/how_to/chatbots_memory/)
  + [How to use example selectors](/docs/how_to/example_selectors/)
  + [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)
  + [How to invoke runnables in parallel](/docs/how_to/parallel/)
  + [How to stream chat model responses](/docs/how_to/chat_streaming/)
  + [How to add default invocation args to a Runnable](/docs/how_to/binding/)
  + [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)
  + [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)
  + [How to do tool/function calling](/docs/how_to/function_calling/)
  + [How to install LangChain packages](/docs/how_to/installation/)
  + [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)
  + [How to use few shot examples](/docs/how_to/few_shot_examples/)
  + [How to run custom functions](/docs/how_to/functions/)
  + [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)
  + [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)
  + [How to route between sub-chains](/docs/how_to/routing/)
  + [How to return structured data from a model](/docs/how_to/structured_output/)
  + [How to summarize text through parallelization](/docs/how_to/summarize_map_reduce/)
  + [How to summarize text through iterative refinement](/docs/how_to/summarize_refine/)
  + [How to summarize text in a single LLM call](/docs/how_to/summarize_stuff/)
  + [How to use toolkits](/docs/how_to/toolkits/)
  + [How to add ad-hoc tool calling capability to LLMs and Chat Models](/docs/how_to/tools_prompting/)
  + [Build an Agent with AgentExecutor (Legacy)](/docs/how_to/agent_executor/)
  + [How to construct knowledge graphs](/docs/how_to/graph_constructing/)
  + [How to partially format prompt templates](/docs/how_to/prompts_partial/)
  + [How to handle multiple queries when doing query analysis](/docs/how_to/query_multiple_queries/)
  + [How to use built-in tools and toolkits](/docs/how_to/tools_builtin/)
  + [How to pass through arguments from one step to the next](/docs/how_to/passthrough/)
  + [How to compose prompts together](/docs/how_to/prompts_composition/)
  + [How to handle multiple retrievers when doing query analysis](/docs/how_to/query_multiple_retrievers/)
  + [How to add values to a chain's state](/docs/how_to/assign/)
  + [How to construct filters for query analysis](/docs/how_to/query_constructing_filters/)
  + [How to configure runtime chain internals](/docs/how_to/configure/)
  + [How deal with high cardinality categoricals when doing query analysis](/docs/how_to/query_high_cardinality/)
  + [Custom Document Loader](/docs/how_to/document_loader_custom/)
  + [How to use the MultiQueryRetriever](/docs/how_to/MultiQueryRetriever/)
  + [How to add scores to retriever results](/docs/how_to/add_scores_retriever/)
  + [Caching](/docs/how_to/caching_embeddings/)
  + [How to use callbacks in async environments](/docs/how_to/callbacks_async/)
  + [How to attach callbacks to a runnable](/docs/how_to/callbacks_attach/)
  + [How to propagate callbacks constructor](/docs/how_to/callbacks_constructor/)
  + [How to dispatch custom callback events](/docs/how_to/callbacks_custom_events/)
  + [How to pass callbacks in at runtime](/docs/how_to/callbacks_runtime/)
  + [How to split by character](/docs/how_to/character_text_splitter/)
  + [How to cache chat model responses](/docs/how_to/chat_model_caching/)
  + [How to handle rate limits](/docs/how_to/chat_model_rate_limiting/)
  + [How to init any model in one line](/docs/how_to/chat_models_universal_init/)
  + [How to track token usage in ChatModels](/docs/how_to/chat_token_usage_tracking/)
  + [How to add tools to chatbots](/docs/how_to/chatbots_tools/)
  + [How to split code](/docs/how_to/code_splitter/)
  + [How to do retrieval with contextual compression](/docs/how_to/contextual_compression/)
  + [How to convert Runnables to Tools](/docs/how_to/convert_runnable_to_tool/)
  + [How to create custom callback handlers](/docs/how_to/custom_callbacks/)
  + [How to create a custom chat model class](/docs/how_to/custom_chat_model/)
  + [Custom Embeddings](/docs/how_to/custom_embeddings/)
  + [How to create a custom LLM class](/docs/how_to/custom_llm/)
  + [Custom Retriever](/docs/how_to/custom_retriever/)
  + [How to create tools](/docs/how_to/custom_tools/)
  + [How to debug your LLM apps](/docs/how_to/debugging/)
  + [How to load CSVs](/docs/how_to/document_loader_csv/)
  + [How to load documents from a directory](/docs/how_to/document_loader_directory/)
  + [How to load HTML](/docs/how_to/document_loader_html/)
  + [How to load JSON](/docs/how_to/document_loader_json/)
  + [How to load Markdown](/docs/how_to/document_loader_markdown/)
  + [How to load Microsoft Office files](/docs/how_to/document_loader_office_file/)
  + [How to load PDFs](/docs/how_to/document_loader_pdf/)
  + [How to load web pages](/docs/how_to/document_loader_web/)
  + [How to create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)
  + [Text embedding models](/docs/how_to/embed_text/)
  + [How to combine results from multiple retrievers](/docs/how_to/ensemble_retriever/)
  + [How to select examples from a LangSmith dataset](/docs/how_to/example_selectors_langsmith/)
  + [How to select examples by length](/docs/how_to/example_selectors_length_based/)
  + [How to select examples by maximal marginal relevance (MMR)](/docs/how_to/example_selectors_mmr/)
  + [How to select examples by n-gram overlap](/docs/how_to/example_selectors_ngram/)
  + [How to select examples by similarity](/docs/how_to/example_selectors_similarity/)
  + [How to use reference examples when doing extraction](/docs/how_to/extraction_examples/)
  + [How to handle long text when doing extraction](/docs/how_to/extraction_long_text/)
  + [How to use prompting alone (no tool calling) to do extraction](/docs/how_to/extraction_parse/)
  + [How to add fallbacks to a runnable](/docs/how_to/fallbacks/)
  + [How to filter messages](/docs/how_to/filter_messages/)
  + [Hybrid Search](/docs/how_to/hybrid/)
  + [How to use the LangChain indexing API](/docs/how_to/indexing/)
  + [How to inspect runnables](/docs/how_to/inspect/)
  + [LangChain Expression Language Cheatsheet](/docs/how_to/lcel_cheatsheet/)
  + [How to cache LLM responses](/docs/how_to/llm_caching/)
  + [How to track token usage for LLMs](/docs/how_to/llm_token_usage_tracking/)
  + [Run models locally](/docs/how_to/local_llms/)
  + [How to get log probabilities](/docs/how_to/logprobs/)
  + [How to reorder retrieved results to mitigate the "lost in the middle" effect](/docs/how_to/long_context_reorder/)
  + [How to split Markdown by Headers](/docs/how_to/markdown_header_metadata_splitter/)
  + [How to merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)
  + [How to add message history](/docs/how_to/message_history/)
  + [How to migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent/)
  + [How to retrieve using multiple vectors per document](/docs/how_to/multi_vector/)
  + [How to pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)
  + [How to use multimodal prompts](/docs/how_to/multimodal_prompts/)
  + [How to create a custom Output Parser](/docs/how_to/output_parser_custom/)
  + [How to use the output-fixing parser](/docs/how_to/output_parser_fixing/)
  + [How to parse JSON output](/docs/how_to/output_parser_json/)
  + [How to retry when a parsing error occurs](/docs/how_to/output_parser_retry/)
  + [How to parse text from message objects](/docs/how_to/output_parser_string/)
  + [How to parse XML output](/docs/how_to/output_parser_xml/)
  + [How to parse YAML output](/docs/how_to/output_parser_yaml/)
  + [How to use the Parent Document Retriever](/docs/how_to/parent_document_retriever/)
  + [How to use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility/)
  + [How to add chat history](/docs/how_to/qa_chat_history_how_to/)
  + [How to get a RAG application to add citations](/docs/how_to/qa_citations/)
  + [How to do per-user retrieval](/docs/how_to/qa_per_user/)
  + [How to get your RAG application to return sources](/docs/how_to/qa_sources/)
  + [How to stream results from your RAG application](/docs/how_to/qa_streaming/)
  + [How to split JSON data](/docs/how_to/recursive_json_splitter/)
  + [How to recursively split text by characters](/docs/how_to/recursive_text_splitter/)
  + [Response metadata](/docs/how_to/response_metadata/)
  + [How to pass runtime secrets to runnables](/docs/how_to/runnable_runtime_secrets/)
  + [How to do "self-querying" retrieval](/docs/how_to/self_query/)
  + [How to split text based on semantic similarity](/docs/how_to/semantic-chunker/)
  + [How to chain runnables](/docs/how_to/sequence/)
  + [How to save and load LangChain objects](/docs/how_to/serialization/)
  + [How to split text by tokens](/docs/how_to/split_by_token/)
  + [How to split HTML](/docs/how_to/split_html/)
  + [How to do question answering over CSVs](/docs/how_to/sql_csv/)
  + [How to deal with large databases when doing SQL question-answering](/docs/how_to/sql_large_db/)
  + [How to better prompt when doing SQL question-answering](/docs/how_to/sql_prompting/)
  + [How to do query validation as part of SQL question-answering](/docs/how_to/sql_query_checking/)
  + [How to stream runnables](/docs/how_to/streaming/)
  + [How to stream responses from an LLM](/docs/how_to/streaming_llm/)
  + [How to use a time-weighted vector store retriever](/docs/how_to/time_weighted_vectorstore/)
  + [How to return artifacts from a tool](/docs/how_to/tool_artifacts/)
  + [How to use chat models to call tools](/docs/how_to/tool_calling/)
  + [How to disable parallel tool calling](/docs/how_to/tool_calling_parallel/)
  + [How to force models to call a tool](/docs/how_to/tool_choice/)
  + [How to access the RunnableConfig from a tool](/docs/how_to/tool_configure/)
  + [How to pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model/)
  + [How to pass run time values to tools](/docs/how_to/tool_runtime/)
  + [How to stream events from a tool](/docs/how_to/tool_stream_events/)
  + [How to stream tool calls](/docs/how_to/tool_streaming/)
  + [How to convert tools to OpenAI Functions](/docs/how_to/tools_as_openai_functions/)
  + [How to handle tool errors](/docs/how_to/tools_error/)
  + [How to use few-shot prompting with tool calling](/docs/how_to/tools_few_shot/)
  + [How to add a human-in-the-loop for tools](/docs/how_to/tools_human/)
  + [How to bind model-specific tools](/docs/how_to/tools_model_specific/)
  + [How to trim messages](/docs/how_to/trim_messages/)
  + [How to create and query vector stores](/docs/how_to/vectorstores/)
* [Conceptual guide](/docs/concepts/)
  + [Agents](/docs/concepts/agents/)
  + [Architecture](/docs/concepts/architecture/)
  + [Async programming with langchain](/docs/concepts/async/)
  + [Callbacks](/docs/concepts/callbacks/)
  + [Chat history](/docs/concepts/chat_history/)
  + [Chat models](/docs/concepts/chat_models/)
  + [Document loaders](/docs/concepts/document_loaders/)
  + [Embedding models](/docs/concepts/embedding_models/)
  + [Evaluation](/docs/concepts/evaluation/)
  + [Example selectors](/docs/concepts/example_selectors/)
  + [Few-shot prompting](/docs/concepts/few_shot_prompting/)
  + [Conceptual guide](/docs/concepts/)
  + [Key-value stores](/docs/concepts/key_value_stores/)
  + [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
  + [Messages](/docs/concepts/messages/)
  + [Multimodality](/docs/concepts/multimodality/)
  + [Output parsers](/docs/concepts/output_parsers/)
  + [Prompt Templates](/docs/concepts/prompt_templates/)
  + [Retrieval augmented generation (RAG)](/docs/concepts/rag/)
  + [Retrieval](/docs/concepts/retrieval/)
  + [Retrievers](/docs/concepts/retrievers/)
  + [Runnable interface](/docs/concepts/runnables/)
  + [Streaming](/docs/concepts/streaming/)
  + [Structured outputs](/docs/concepts/structured_outputs/)
  + [Testing](/docs/concepts/testing/)
  + [String-in, string-out llms](/docs/concepts/text_llms/)
  + [Text splitters](/docs/concepts/text_splitters/)
  + [Tokens](/docs/concepts/tokens/)
  + [Tool calling](/docs/concepts/tool_calling/)
  + [Tools](/docs/concepts/tools/)
  + [Tracing](/docs/concepts/tracing/)
  + [Vector stores](/docs/concepts/vectorstores/)
  + [Why LangChain?](/docs/concepts/why_langchain/)
* Ecosystem
  + [ü¶úüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/)
  + [ü¶úüï∏Ô∏è LangGraph](https://langchain-ai.github.io/langgraph/)
* Versions
  + [v0.3](/docs/versions/v0_3/)
  + [v0.2](/docs/versions/v0_2/overview/)
  + [Pydantic compatibility](/docs/how_to/pydantic_compatibility/)
  + [Migrating from v0.0 chains](/docs/versions/migrating_chains/)
    - [How to migrate from v0.0 chains](/docs/versions/migrating_chains/)
    - [Migrating from ConstitutionalChain](/docs/versions/migrating_chains/constitutional_chain/)
    - [Migrating from ConversationalChain](/docs/versions/migrating_chains/conversation_chain/)
    - [Migrating from ConversationalRetrievalChain](/docs/versions/migrating_chains/conversation_retrieval_chain/)
    - [Migrating from LLMChain](/docs/versions/migrating_chains/llm_chain/)
    - [Migrating from LLMMathChain](/docs/versions/migrating_chains/llm_math_chain/)
    - [Migrating from LLMRouterChain](/docs/versions/migrating_chains/llm_router_chain/)
    - [Migrating from MapReduceDocumentsChain](/docs/versions/migrating_chains/map_reduce_chain/)
    - [Migrating from MapRerankDocumentsChain](/docs/versions/migrating_chains/map_rerank_docs_chain/)
    - [Migrating from MultiPromptChain](/docs/versions/migrating_chains/multi_prompt_chain/)
    - [Migrating from RefineDocumentsChain](/docs/versions/migrating_chains/refine_docs_chain/)
    - [Migrating from RetrievalQA](/docs/versions/migrating_chains/retrieval_qa/)
    - [Migrating from StuffDocumentsChain](/docs/versions/migrating_chains/stuff_docs_chain/)
  + [Upgrading to LangGraph memory](/docs/versions/migrating_memory/)
    - [How to migrate to LangGraph memory](/docs/versions/migrating_memory/)
    - [How to use BaseChatMessageHistory with LangGraph](/docs/versions/migrating_memory/chat_history/)
    - [Migrating off ConversationBufferMemory or ConversationStringBufferMemory](/docs/versions/migrating_memory/conversation_buffer_memory/)
    - [Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory](/docs/versions/migrating_memory/conversation_buffer_window_memory/)
    - [Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory](/docs/versions/migrating_memory/conversation_summary_memory/)
    - [A Long-Term Memory Agent](/docs/versions/migrating_memory/long_term_memory_agent/)
  + [Release policy](/docs/versions/release_policy/)
* [Security Policy](/docs/security/)


* Conceptual guide
On this page[![](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/index.mdx)

Conceptual guide
================

This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.

We recommend that you go through at least one of the [Tutorials](/docs/tutorials/) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.

The conceptual guide does not cover step-by-step instructions or specific implementation examples ‚Äî those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials/). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).

High level[‚Äã](#high-level)
--------------------------

* **[Why LangChain?](/docs/concepts/why_langchain/)**: Overview of the value that LangChain provides.
* **[Architecture](/docs/concepts/architecture/)**: How packages are organized in the LangChain ecosystem.

Concepts[‚Äã](#concepts)
----------------------

* **[Chat models](/docs/concepts/chat_models/)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.
* **[Messages](/docs/concepts/messages/)**: The unit of communication in chat models, used to represent model input and output.
* **[Chat history](/docs/concepts/chat_history/)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.
* **[Tools](/docs/concepts/tools/)**: A function with an associated schema defining the function's name, description, and the arguments it accepts.
* **[Tool calling](/docs/concepts/tool_calling/)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.
* **[Structured output](/docs/concepts/structured_outputs/)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.
* **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.
* **[Multimodality](/docs/concepts/multimodality/)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.
* **[Runnable interface](/docs/concepts/runnables/)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.
* **[Streaming](/docs/concepts/streaming/)**: LangChain streaming APIs for surfacing results as they are generated.
* **[LangChain Expression Language (LCEL)](/docs/concepts/lcel/)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.
* **[Document loaders](/docs/concepts/document_loaders/)**: Load a source as a list of documents.
* **[Retrieval](/docs/concepts/retrieval/)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.
* **[Text splitters](/docs/concepts/text_splitters/)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.
* **[Embedding models](/docs/concepts/embedding_models/)**: Models that represent data such as text or images in a vector space.
* **[Vector stores](/docs/concepts/vectorstores/)**: Storage of and efficient search over vectors and associated metadata.
* **[Retriever](/docs/concepts/retrievers/)**: A component that returns relevant documents from a knowledge base in response to a query.
* **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag/)**: A technique that enhances language models by combining them with external knowledge bases.
* **[Agents](/docs/concepts/agents/)**: Use a [language model](/docs/concepts/chat_models/) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools/).
* **[Prompt templates](/docs/concepts/prompt_templates/)**: Component for factoring out the static parts of a model "prompt" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.
* **[Output parsers](/docs/concepts/output_parsers/)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).
* **[Few-shot prompting](/docs/concepts/few_shot_prompting/)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.
* **[Example selectors](/docs/concepts/example_selectors/)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.
* **[Async programming](/docs/concepts/async/)**: The basics that one should know to use LangChain in an asynchronous context.
* **[Callbacks](/docs/concepts/callbacks/)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.
* **[Tracing](/docs/concepts/tracing/)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.
* **[Evaluation](/docs/concepts/evaluation/)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.
* **[Testing](/docs/concepts/testing/)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.

Glossary[‚Äã](#glossary)
----------------------

* **[AIMessageChunk](/docs/concepts/messages/#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.
* **[AIMessage](/docs/concepts/messages/#aimessage)**: Represents a complete response from an AI model.
* **[astream\_events](/docs/concepts/chat_models/#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel/) chains.
* **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.
* **[batch](/docs/concepts/runnables/)**: Use to execute a runnable with batch inputs.
* **[bind\_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.
* **[Caching](/docs/concepts/chat_models/#caching)**: Storing results to avoid redundant calls to a chat model.
* **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.
* **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.
* **[Context window](/docs/concepts/chat_models/#context-window)**: The maximum size of input a chat model can process.
* **[Conversation patterns](/docs/concepts/chat_history/#conversation-patterns)**: Common patterns in chat interactions.
* **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain's representation of a document.
* **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.
* **[HumanMessage](/docs/concepts/messages/#humanmessage)**: Represents a message from a human user.
* **[InjectedState](/docs/concepts/tools/#injectedstate)**: A state injected into a tool function.
* **[InjectedStore](/docs/concepts/tools/#injectedstore)**: A store that can be injected into a tool for data persistence.
* **[InjectedToolArg](/docs/concepts/tools/#injectedtoolarg)**: Mechanism to inject arguments into tool functions.
* **[input and output types](/docs/concepts/runnables/#input-and-output-types)**: Types used for input and output in Runnables.
* **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.
* **[Integration tests](/docs/concepts/testing/#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.
* **[invoke](/docs/concepts/runnables/)**: A standard method to invoke a Runnable.
* **[JSON mode](/docs/concepts/structured_outputs/#json-mode)**: Returning responses in JSON format.
* **[langchain-community](/docs/concepts/architecture/#langchain-community)**: Community-driven components for LangChain.
* **[langchain-core](/docs/concepts/architecture/#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.
* **[langchain](/docs/concepts/architecture/#langchain)**: A package for higher level components (e.g., some pre-built chains).
* **[langgraph](/docs/concepts/architecture/#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.
* **[langserve](/docs/concepts/architecture/#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.
* **[LLMs (legacy)](/docs/concepts/text_llms/)**: Older language models that take a string as input and return a string as output.
* **[Managing chat history](/docs/concepts/chat_history/#managing-chat-history)**: Techniques to maintain and manage the chat history.
* **[OpenAI format](/docs/concepts/messages/#openai-format)**: OpenAI's message format for chat models.
* **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.
* **[rate-limiting](/docs/concepts/chat_models/#rate-limiting)**: Client side rate limiting for chat models.
* **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.
* **[role](/docs/concepts/messages/#role)**: Represents the role (e.g., user, assistant) of a chat message.
* **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).
* **[Standard parameters for chat models](/docs/concepts/chat_models/#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.
* **[Standard tests](/docs/concepts/testing/#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.
* **[stream](/docs/concepts/streaming/)**: Use to stream output from a Runnable or a graph.
* **[Tokenization](/docs/concepts/tokens/)**: The process of converting data into tokens and vice versa.
* **[Tokens](/docs/concepts/tokens/)**: The basic unit that a language model reads, processes, and generates under the hood.
* **[Tool artifacts](/docs/concepts/tools/#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.
* **[Tool binding](/docs/concepts/tool_calling/#tool-binding)**: Binding tools to models.
* **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.
* **[Toolkits](/docs/concepts/tools/#toolkits)**: A collection of tools that can be used together.
* **[ToolMessage](/docs/concepts/messages/#toolmessage)**: Represents a message that contains the results of a tool execution.
* **[Unit tests](/docs/concepts/testing/#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.
* **[Vector stores](/docs/concepts/vectorstores/)**: Datastores specialized for storing and efficiently searching vector embeddings.
* **[with\_structured\_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling/) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.
* **[with\_types](/docs/concepts/runnables/#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.
[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/index.mdx)

---

#### Was this page helpful?

[PreviousHow to create and query vector stores](/docs/how_to/vectorstores/)[NextAgents](/docs/concepts/agents/)

* [High level](#high-level)
* [Concepts](#concepts)
* [Glossary](#glossary)
Community

* [Twitter](https://twitter.com/LangChainAI)
GitHub

* [Organization](https://github.com/langchain-ai)
* [Python](https://github.com/langchain-ai/langchain)
* [JS/TS](https://github.com/langchain-ai/langchainjs)
More

* [Homepage](https://langchain.com)
* [Blog](https://blog.langchain.dev)
* [YouTube](https://www.youtube.com/@LangChain)
Copyright ¬© 2024 LangChain, Inc.