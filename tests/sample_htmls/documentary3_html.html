<!DOCTYPE html><html class="writer-html5" lang="en"><head>
  <meta charset="utf-8"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Downloader Middleware — Scrapy 2.12.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css">
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css">
      <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css">
      <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css">
      <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css">
      <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css">
      <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css">
      <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css">
      <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css">
      <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css">
      <link rel="stylesheet" type="text/css" href="../_static/css/sphinx_rtd_theme.css">
      <link rel="stylesheet" type="text/css" href="../_static/custom.css">

  
    <link rel="canonical" href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html">
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/js/hoverxref.js"></script>
        <script src="../_static/js/tooltipster.bundle.min.js"></script>
        <script src="../_static/js/micromodal.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html">
    <link rel="search" title="Search" href="../search.html">
    <link rel="next" title="Spider Middleware" href="spider-middleware.html">
    <link rel="prev" title="Add-ons" href="addons.html"> 
<script async="" type="text/javascript" src="/_/static/javascript/readthedocs-addons.js"></script><meta name="readthedocs-project-slug" content="scrapy"><meta name="readthedocs-version-slug" content="latest"><meta name="readthedocs-resolver-filename" content="/topics/downloader-middleware.html"><meta name="readthedocs-http-status" content="200"></head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">

          
          
          <a href="../index.html" class="icon icon-home">
            Scrapy
          </a>
              <div class="version">
                2.12
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs">
    <input type="hidden" name="check_keywords" value="yes">
    <input type="hidden" name="area" value="default">
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">First steps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/overview.html">Scrapy at a glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/install.html">Installation guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/tutorial.html">Scrapy Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="commands.html">Command line tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="spiders.html">Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="selectors.html">Selectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="items.html">Items</a></li>
<li class="toctree-l1"><a class="reference internal" href="loaders.html">Item Loaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="shell.html">Scrapy shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="item-pipeline.html">Item Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="feed-exports.html">Feed exports</a></li>
<li class="toctree-l1"><a class="reference internal" href="request-response.html">Requests and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="link-extractors.html">Link Extractors</a></li>
<li class="toctree-l1"><a class="reference internal" href="settings.html">Settings</a></li>
<li class="toctree-l1"><a class="reference internal" href="exceptions.html">Exceptions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Built-in services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="stats.html">Stats Collection</a></li>
<li class="toctree-l1"><a class="reference internal" href="email.html">Sending e-mail</a></li>
<li class="toctree-l1"><a class="reference internal" href="telnetconsole.html">Telnet Console</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Solving specific problems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug.html">Debugging Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="contracts.html">Spiders Contracts</a></li>
<li class="toctree-l1"><a class="reference internal" href="practices.html">Common Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="broad-crawls.html">Broad Crawls</a></li>
<li class="toctree-l1"><a class="reference internal" href="developer-tools.html">Using your browser’s Developer Tools for scraping</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic-content.html">Selecting dynamically-loaded content</a></li>
<li class="toctree-l1"><a class="reference internal" href="leaks.html">Debugging memory leaks</a></li>
<li class="toctree-l1"><a class="reference internal" href="media-pipeline.html">Downloading and processing files and images</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">Deploying Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="autothrottle.html">AutoThrottle extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="jobs.html">Jobs: pausing and resuming crawls</a></li>
<li class="toctree-l1"><a class="reference internal" href="coroutines.html">Coroutines</a></li>
<li class="toctree-l1"><a class="reference internal" href="asyncio.html">asyncio</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extending Scrapy</span></p>
<ul class="current" aria-expanded="true">
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="addons.html">Add-ons</a></li>
<li class="toctree-l1 current" aria-expanded="true"><a class="reference internal current" href="#" aria-expanded="true"><button class="toctree-expand" title="Open/close menu"></button>Downloader Middleware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#activating-a-downloader-middleware">Activating a downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="#writing-your-own-downloader-middleware"><button class="toctree-expand" title="Open/close menu"></button>Writing your own downloader middleware</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware"><button class="toctree-expand" title="Open/close menu"></button><code class="docutils literal notranslate"><span class="pre">DownloaderMiddleware</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="docutils literal notranslate"><span class="pre">DownloaderMiddleware.process_request()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="docutils literal notranslate"><span class="pre">DownloaderMiddleware.process_response()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="docutils literal notranslate"><span class="pre">DownloaderMiddleware.process_exception()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler"><code class="docutils literal notranslate"><span class="pre">DownloaderMiddleware.from_crawler()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#built-in-downloader-middleware-reference"><button class="toctree-expand" title="Open/close menu"></button>Built-in downloader middleware reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.cookies"><button class="toctree-expand" title="Open/close menu"></button>CookiesMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.cookies.CookiesMiddleware"><code class="docutils literal notranslate"><span class="pre">CookiesMiddleware</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#multiple-cookie-sessions-per-spider">Multiple cookie sessions per spider</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cookies-enabled">COOKIES_ENABLED</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cookies-debug">COOKIES_DEBUG</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.defaultheaders"><button class="toctree-expand" title="Open/close menu"></button>DefaultHeadersMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware"><code class="docutils literal notranslate"><span class="pre">DefaultHeadersMiddleware</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.downloadtimeout"><button class="toctree-expand" title="Open/close menu"></button>DownloadTimeoutMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware"><code class="docutils literal notranslate"><span class="pre">DownloadTimeoutMiddleware</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.httpauth"><button class="toctree-expand" title="Open/close menu"></button>HttpAuthMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"><code class="docutils literal notranslate"><span class="pre">HttpAuthMiddleware</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.httpcache"><button class="toctree-expand" title="Open/close menu"></button>HttpCacheMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"><code class="docutils literal notranslate"><span class="pre">HttpCacheMiddleware</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#dummy-policy-default">Dummy policy (default)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rfc2616-policy">RFC2616 policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#filesystem-storage-backend-default">Filesystem storage backend (default)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dbm-storage-backend">DBM storage backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#writing-your-own-storage-backend">Writing your own storage backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#httpcache-middleware-settings">HTTPCache middleware settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.httpcompression"><button class="toctree-expand" title="Open/close menu"></button>HttpCompressionMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"><code class="docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#httpcompressionmiddleware-settings">HttpCompressionMiddleware Settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.httpproxy"><button class="toctree-expand" title="Open/close menu"></button>HttpProxyMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.offsite"><button class="toctree-expand" title="Open/close menu"></button>OffsiteMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.offsite.OffsiteMiddleware"><code class="docutils literal notranslate"><span class="pre">OffsiteMiddleware</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.redirect"><button class="toctree-expand" title="Open/close menu"></button>RedirectMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#redirectmiddleware-settings">RedirectMiddleware settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#metarefreshmiddleware"><button class="toctree-expand" title="Open/close menu"></button>MetaRefreshMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"><code class="docutils literal notranslate"><span class="pre">MetaRefreshMiddleware</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#metarefreshmiddleware-settings">MetaRefreshMiddleware settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.retry"><button class="toctree-expand" title="Open/close menu"></button>RetryMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.retry.get_retry_request"><code class="docutils literal notranslate"><span class="pre">get_retry_request()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#retrymiddleware-settings">RetryMiddleware Settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.robotstxt"><button class="toctree-expand" title="Open/close menu"></button>RobotsTxtMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware"><code class="docutils literal notranslate"><span class="pre">RobotsTxtMiddleware</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#protego-parser">Protego parser</a></li>
<li class="toctree-l4"><a class="reference internal" href="#robotfileparser">RobotFileParser</a></li>
<li class="toctree-l4"><a class="reference internal" href="#robotexclusionrulesparser">Robotexclusionrulesparser</a></li>
<li class="toctree-l4"><a class="reference internal" href="#implementing-support-for-a-new-parser">Implementing support for a new parser</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.stats"><button class="toctree-expand" title="Open/close menu"></button>DownloaderStats</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.stats.DownloaderStats"><code class="docutils literal notranslate"><span class="pre">DownloaderStats</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.useragent"><button class="toctree-expand" title="Open/close menu"></button>UserAgentMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware"><code class="docutils literal notranslate"><span class="pre">UserAgentMiddleware</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.downloadermiddlewares.ajaxcrawl"><button class="toctree-expand" title="Open/close menu"></button>AjaxCrawlMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware"><code class="docutils literal notranslate"><span class="pre">AjaxCrawlMiddleware</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#ajaxcrawlmiddleware-settings">AjaxCrawlMiddleware Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="#httpproxymiddleware-settings">HttpProxyMiddleware settings</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="spider-middleware.html">Spider Middleware</a></li>
<li class="toctree-l1"><a class="reference internal" href="extensions.html">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="signals.html">Signals</a></li>
<li class="toctree-l1"><a class="reference internal" href="scheduler.html">Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="exporters.html">Item Exporters</a></li>
<li class="toctree-l1"><a class="reference internal" href="components.html">Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Core API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">All the rest</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to Scrapy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../versioning.html">Versioning and API stability</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Scrapy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Downloader Middleware</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/topics/downloader-middleware.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="downloader-middleware">
<span id="topics-downloader-middleware"></span><h1>Downloader Middleware<a class="headerlink" href="#downloader-middleware" title="Permalink to this heading">¶</a></h1>
<p>The downloader middleware is a framework of hooks into Scrapy’s
request/response processing.  It’s a light, low-level system for globally
altering Scrapy’s requests and responses.</p>
<section id="activating-a-downloader-middleware">
<span id="topics-downloader-middleware-setting"></span><h2>Activating a downloader middleware<a class="headerlink" href="#activating-a-downloader-middleware" title="Permalink to this heading">¶</a></h2>
<p>To activate a downloader middleware component, add it to the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the
middleware class paths and their values are the middleware orders.</p>
<p>Here’s an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"myproject.middlewares.CustomDownloaderMiddleware"</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting is merged with the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list of
enabled middlewares: the first middleware is the one closer to the engine and
the last is the one closer to the downloader. In other words,
the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a>
method of each middleware will be invoked in increasing
middleware order (100, 200, 300, …) and the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a> method
of each middleware will be invoked in decreasing order.</p>
<p>To decide which order to assign to your middleware see the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to
where you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a built-in middleware (the ones defined in
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> and enabled by default) you must define it
in your project’s <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting and assign <code class="docutils literal notranslate"><span class="pre">None</span></code>
as its value.  For example, if you want to disable the user-agent middleware:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"myproject.middlewares.CustomDownloaderMiddleware"</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
    <span class="s2">"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</section>
<section id="writing-your-own-downloader-middleware">
<span id="topics-downloader-middleware-custom"></span><h2>Writing your own downloader middleware<a class="headerlink" href="#writing-your-own-downloader-middleware" title="Permalink to this heading">¶</a></h2>
<p>Each downloader middleware is a Python class that defines one or more of the
methods defined below.</p>
<p>The main entry point is the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> class method, which receives a
<a class="reference internal" href="api.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance. The <a class="reference internal" href="api.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
object gives you access, for example, to the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#topics-settings"><span class="std std-ref">settings</span></a>.</p>
<span class="target" id="module-scrapy.downloadermiddlewares"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.DownloaderMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.</span></span><span class="sig-name descname"><span class="pre">DownloaderMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Any of the downloader middleware methods may also return a deferred.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request">
<span class="sig-name descname"><span class="pre">process_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each request that goes through the download
middleware.</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> should either: return <code class="docutils literal notranslate"><span class="pre">None</span></code>, return a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code> object, return a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>
object, or raise <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a>.</p>
<p>If it returns <code class="docutils literal notranslate"><span class="pre">None</span></code>, Scrapy will continue processing this request, executing all
other middlewares until, finally, the appropriate downloader handler is called
the request performed (and its response downloaded).</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object, Scrapy won’t bother
calling <em>any</em> other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> or <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods,
or the appropriate download function; it’ll return that response. The <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a>
methods of installed middleware is always called on every response.</p>
<p>If it returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object, Scrapy will stop calling
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> methods and reschedule the returned request. Once the newly returned
request is performed, the appropriate middleware chain will be called on
the downloaded response.</p>
<p>If it raises an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception, the
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of installed downloader middleware will be called.
If none of them handle the exception, the errback function of the request
(<code class="docutils literal notranslate"><span class="pre">Request.errback</span></code>) is called. If no code handles the raised exception, it is
ignored and not logged (unlike other exceptions).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request being processed</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this request is intended</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response">
<span class="sig-name descname"><span class="pre">process_response</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a> should either: return a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
object, return a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object or
raise a <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> (it could be the same given
response, or a brand-new one), that response will continue to be processed
with the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a> of the next middleware in the chain.</p>
<p>If it returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object, the middleware chain is
halted and the returned request is rescheduled to be downloaded in the future.
This is the same behavior as if a request is returned from <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a>.</p>
<p>If it raises an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception, the errback
function of the request (<code class="docutils literal notranslate"><span class="pre">Request.errback</span></code>) is called. If no code handles the raised
exception, it is ignored and not logged (unlike other exceptions).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request</strong> (is a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that originated the response</p></li>
<li><p><strong>response</strong> (<a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response being processed</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this response is intended</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception">
<span class="sig-name descname"><span class="pre">process_exception</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exception</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>Scrapy calls <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> when a download handler
or a <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> (from a downloader middleware) raises an
exception (including an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception)</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> should return: either <code class="docutils literal notranslate"><span class="pre">None</span></code>,
a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object, or a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object.</p>
<p>If it returns <code class="docutils literal notranslate"><span class="pre">None</span></code>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of installed middleware,
until no middleware is left and the default exception handling kicks in.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object, the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a>
method chain of installed middleware is started, and Scrapy won’t bother calling
any other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of middleware.</p>
<p>If it returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object, the returned request is
rescheduled to be downloaded in the future. This stops the execution of
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of the middleware the same as returning a
response would.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request</strong> (is a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that generated the exception</p></li>
<li><p><strong>exception</strong> (an <code class="docutils literal notranslate"><span class="pre">Exception</span></code> object) – the raised exception</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this request is intended</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler">
<span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crawler</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>If present, this classmethod is called to create a middleware instance
from a <a class="reference internal" href="api.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>. It must return a new instance
of the middleware. Crawler object provides access to all Scrapy core
components like settings and signals; it is a way for middleware to
access them and hook its functionality into Scrapy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>crawler</strong> (<a class="reference internal" href="api.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object) – crawler that uses this middleware</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="built-in-downloader-middleware-reference">
<span id="topics-downloader-middleware-ref"></span><h2>Built-in downloader middleware reference<a class="headerlink" href="#built-in-downloader-middleware-reference" title="Permalink to this heading">¶</a></h2>
<p>This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own downloader
middleware, see the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#topics-downloader-middleware"><span class="std std-ref">downloader middleware usage guide</span></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting.</p>
<section id="module-scrapy.downloadermiddlewares.cookies">
<span id="cookiesmiddleware"></span><span id="cookies-mw"></span><h3>CookiesMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.cookies" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.cookies.</span></span><span class="sig-name descname"><span class="pre">CookiesMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/cookies.html#CookiesMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware enables working with sites that require cookies, such as
those that use sessions. It keeps track of cookies sent by web servers, and
sends them back on subsequent requests (from that spider), just like web
browsers do.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>When non-UTF8 encoded byte sequences are passed to a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>, the <code class="docutils literal notranslate"><span class="pre">CookiesMiddleware</span></code> will log
a warning. Refer to <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="logging.html#topics-logging-advanced-customization"><span class="std std-ref">Advanced customization</span></a>
to customize the logging behaviour.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Cookies set via the <code class="docutils literal notranslate"><span class="pre">Cookie</span></code> header are not considered by the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>. If you need to set cookies for a request, use the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request.cookies</span></code> parameter. This is a known
current limitation that is being worked on.</p>
</div>
</dd></dl>

<p>The following settings can be used to configure the cookie middleware:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_DEBUG</span></code></a></p></li>
</ul>
<section id="multiple-cookie-sessions-per-spider">
<span id="std-reqmeta-cookiejar"></span><h4>Multiple cookie sessions per spider<a class="headerlink" href="#multiple-cookie-sessions-per-spider" title="Permalink to this heading">¶</a></h4>
<p>There is support for keeping multiple cookie sessions per spider by using the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">cookiejar</span></code></a> Request meta key. By default it uses a single cookie jar
(session), but you can pass an identifier to use different ones.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">url</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">urls</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s2">"cookiejar"</span><span class="p">:</span> <span class="n">i</span><span class="p">},</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page</span><span class="p">)</span>
</pre></div>
</div>
<p>Keep in mind that the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">cookiejar</span></code></a> meta key is not “sticky”. You need to keep
passing it along on subsequent requests. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c1"># do some processing</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span>
        <span class="s2">"http://www.example.com/otherpage"</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s2">"cookiejar"</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">"cookiejar"</span><span class="p">]},</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_other_page</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="cookies-enabled">
<span id="std-setting-COOKIES_ENABLED"></span><h4>COOKIES_ENABLED<a class="headerlink" href="#cookies-enabled" title="Permalink to this heading">¶</a></h4>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable the cookies middleware. If disabled, no cookies will be sent
to web servers.</p>
<p>Notice that despite the value of <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span></code></a> setting if
<code class="docutils literal notranslate"><span class="pre">Request.</span></code><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="request-response.html#std-reqmeta-dont_merge_cookies"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">meta['dont_merge_cookies']</span></code></a>
evaluates to <code class="docutils literal notranslate"><span class="pre">True</span></code> the request cookies will <strong>not</strong> be sent to the
web server and received cookies in <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> will
<strong>not</strong> be merged with the existing cookies.</p>
<p>For more detailed information see the <code class="docutils literal notranslate"><span class="pre">cookies</span></code> parameter in
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>.</p>
</section>
<section id="cookies-debug">
<span id="std-setting-COOKIES_DEBUG"></span><h4>COOKIES_DEBUG<a class="headerlink" href="#cookies-debug" title="Permalink to this heading">¶</a></h4>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, Scrapy will log all cookies sent in requests (i.e. <code class="docutils literal notranslate"><span class="pre">Cookie</span></code>
header) and all cookies received in responses (i.e. <code class="docutils literal notranslate"><span class="pre">Set-Cookie</span></code> header).</p>
<p>Here’s an example of a log with <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_DEBUG</span></code></a> enabled:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">35</span><span class="p">:</span><span class="mi">10</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Spider</span> <span class="n">opened</span>
<span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">35</span><span class="p">:</span><span class="mi">10</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">downloadermiddlewares</span><span class="o">.</span><span class="n">cookies</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Sending</span> <span class="n">cookies</span> <span class="n">to</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">diningcity</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">netherlands</span><span class="o">/</span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span>
        <span class="n">Cookie</span><span class="p">:</span> <span class="n">clientlanguage_nl</span><span class="o">=</span><span class="n">en_EN</span>
<span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">35</span><span class="p">:</span><span class="mi">14</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">downloadermiddlewares</span><span class="o">.</span><span class="n">cookies</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Received</span> <span class="n">cookies</span> <span class="n">from</span><span class="p">:</span> <span class="o">&lt;</span><span class="mi">200</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">diningcity</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">netherlands</span><span class="o">/</span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span>
        <span class="n">Set</span><span class="o">-</span><span class="n">Cookie</span><span class="p">:</span> <span class="n">JSESSIONID</span><span class="o">=</span><span class="n">B</span><span class="o">~</span><span class="n">FA4DC0C496C8762AE4F1A620EAB34F38</span><span class="p">;</span> <span class="n">Path</span><span class="o">=/</span>
        <span class="n">Set</span><span class="o">-</span><span class="n">Cookie</span><span class="p">:</span> <span class="n">ip_isocode</span><span class="o">=</span><span class="n">US</span>
        <span class="n">Set</span><span class="o">-</span><span class="n">Cookie</span><span class="p">:</span> <span class="n">clientlanguage_nl</span><span class="o">=</span><span class="n">en_EN</span><span class="p">;</span> <span class="n">Expires</span><span class="o">=</span><span class="n">Thu</span><span class="p">,</span> <span class="mi">07</span><span class="o">-</span><span class="n">Apr</span><span class="o">-</span><span class="mi">2011</span> <span class="mi">21</span><span class="p">:</span><span class="mi">21</span><span class="p">:</span><span class="mi">34</span> <span class="n">GMT</span><span class="p">;</span> <span class="n">Path</span><span class="o">=/</span>
<span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">49</span><span class="p">:</span><span class="mi">50</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">diningcity</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">netherlands</span><span class="o">/</span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="module-scrapy.downloadermiddlewares.defaultheaders">
<span id="defaultheadersmiddleware"></span><h3>DefaultHeadersMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.defaultheaders" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.defaultheaders.</span></span><span class="sig-name descname"><span class="pre">DefaultHeadersMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/defaultheaders.html#DefaultHeadersMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets all default requests headers specified in the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-DEFAULT_REQUEST_HEADERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.downloadtimeout">
<span id="downloadtimeoutmiddleware"></span><h3>DownloadTimeoutMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.downloadtimeout" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.downloadtimeout.</span></span><span class="sig-name descname"><span class="pre">DownloadTimeoutMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/downloadtimeout.html#DownloadTimeoutMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the download timeout for requests specified in the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-DOWNLOAD_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_TIMEOUT</span></code></a> setting or <code class="xref py py-attr docutils literal notranslate"><span class="pre">download_timeout</span></code>
spider attribute.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can also set download timeout per-request using
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="request-response.html#std-reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_timeout</span></code></a> Request.meta key; this is supported
even when DownloadTimeoutMiddleware is disabled.</p>
</div>
</section>
<section id="module-scrapy.downloadermiddlewares.httpauth">
<span id="httpauthmiddleware"></span><h3>HttpAuthMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpauth" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.httpauth.</span></span><span class="sig-name descname"><span class="pre">HttpAuthMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/httpauth.html#HttpAuthMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware authenticates all requests generated from certain spiders
using <a class="reference external" href="https://en.wikipedia.org/wiki/Basic_access_authentication">Basic access authentication</a> (aka. HTTP auth).</p>
<p>To enable HTTP authentication for a spider, set the <code class="docutils literal notranslate"><span class="pre">http_user</span></code> and
<code class="docutils literal notranslate"><span class="pre">http_pass</span></code> spider attributes to the authentication data and the
<code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> spider attribute to the domain which requires this
authentication (its subdomains will be also handled in the same way).
You can set <code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> to <code class="docutils literal notranslate"><span class="pre">None</span></code> to enable the
authentication for all requests but you risk leaking your authentication
credentials to unrelated domains.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>In previous Scrapy versions HttpAuthMiddleware sent the authentication
data with all requests, which is a security problem if the spider
makes requests to several different domains. Currently if the
<code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> attribute is not set, the middleware will use the
domain of the first request, which will work for some spiders but not
for others. In the future the middleware will produce an error instead.</p>
</div>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span>


<span class="k">class</span> <span class="nc">SomeIntranetSiteSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">http_user</span> <span class="o">=</span> <span class="s2">"someuser"</span>
    <span class="n">http_pass</span> <span class="o">=</span> <span class="s2">"somepass"</span>
    <span class="n">http_auth_domain</span> <span class="o">=</span> <span class="s2">"intranet.example.com"</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">"intranet.example.com"</span>

    <span class="c1"># .. rest of the spider code omitted ...</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.httpcache">
<span id="httpcachemiddleware"></span><h3>HttpCacheMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpcache" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.httpcache.</span></span><span class="sig-name descname"><span class="pre">HttpCacheMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/httpcache.html#HttpCacheMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware provides low-level cache to all HTTP requests and responses.
It has to be combined with a cache storage backend as well as a cache policy.</p>
<p>Scrapy ships with the following HTTP cache storage backends:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#httpcache-storage-fs"><span class="std std-ref">Filesystem storage backend (default)</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#httpcache-storage-dbm"><span class="std std-ref">DBM storage backend</span></a></p></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache storage backend with the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code></a>
setting. Or you can also <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#httpcache-storage-custom"><span class="std std-ref">implement your own storage backend.</span></a></p>
<p>Scrapy ships with two HTTP cache policies:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#httpcache-policy-rfc2616"><span class="std std-ref">RFC2616 policy</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#httpcache-policy-dummy"><span class="std std-ref">Dummy policy (default)</span></a></p></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache policy with the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_POLICY</span></code></a>
setting. Or you can also implement your own policy.</p>
<p id="std-reqmeta-dont_cache">You can also avoid caching a response on every policy using <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-reqmeta-dont_cache"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_cache</span></code></a> meta key equals <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd></dl>

<section id="dummy-policy-default">
<span id="httpcache-policy-dummy"></span><h4>Dummy policy (default)<a class="headerlink" href="#dummy-policy-default" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.DummyPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.httpcache.</span></span><span class="sig-name descname"><span class="pre">DummyPolicy</span></span><a class="reference internal" href="../_modules/scrapy/extensions/httpcache.html#DummyPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.extensions.httpcache.DummyPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>This policy has no awareness of any HTTP Cache-Control directives.
Every request and its corresponding response are cached.  When the same
request is seen again, the response is returned without transferring
anything from the Internet.</p>
<p>The Dummy policy is useful for testing spiders faster (without having
to wait for downloads every time) and for trying your spider offline,
when an Internet connection is not available. The goal is to be able to
“replay” a spider run <em>exactly as it ran before</em>.</p>
</dd></dl>

</section>
<section id="rfc2616-policy">
<span id="httpcache-policy-rfc2616"></span><h4>RFC2616 policy<a class="headerlink" href="#rfc2616-policy" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.RFC2616Policy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.httpcache.</span></span><span class="sig-name descname"><span class="pre">RFC2616Policy</span></span><a class="reference internal" href="../_modules/scrapy/extensions/httpcache.html#RFC2616Policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.extensions.httpcache.RFC2616Policy" title="Permalink to this definition">¶</a></dt>
<dd><p>This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and speed up
crawls).</p>
<p>What is implemented:</p>
<ul class="simple">
<li><p>Do not attempt to store responses/requests with <code class="docutils literal notranslate"><span class="pre">no-store</span></code> cache-control directive set</p></li>
<li><p>Do not serve responses from cache if <code class="docutils literal notranslate"><span class="pre">no-cache</span></code> cache-control directive is set even for fresh responses</p></li>
<li><p>Compute freshness lifetime from <code class="docutils literal notranslate"><span class="pre">max-age</span></code> cache-control directive</p></li>
<li><p>Compute freshness lifetime from <code class="docutils literal notranslate"><span class="pre">Expires</span></code> response header</p></li>
<li><p>Compute freshness lifetime from <code class="docutils literal notranslate"><span class="pre">Last-Modified</span></code> response header (heuristic used by Firefox)</p></li>
<li><p>Compute current age from <code class="docutils literal notranslate"><span class="pre">Age</span></code> response header</p></li>
<li><p>Compute current age from <code class="docutils literal notranslate"><span class="pre">Date</span></code> header</p></li>
<li><p>Revalidate stale responses based on <code class="docutils literal notranslate"><span class="pre">Last-Modified</span></code> response header</p></li>
<li><p>Revalidate stale responses based on <code class="docutils literal notranslate"><span class="pre">ETag</span></code> response header</p></li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">Date</span></code> header for any received response missing it</p></li>
<li><p>Support <code class="docutils literal notranslate"><span class="pre">max-stale</span></code> cache-control directive in requests</p></li>
</ul>
<p>This allows spiders to be configured with the full RFC2616 cache policy,
but avoid revalidation on a request-by-request basis, while remaining
conformant with the HTTP spec.</p>
<p>Example:</p>
<p>Add <code class="docutils literal notranslate"><span class="pre">Cache-Control:</span> <span class="pre">max-stale=600</span></code> to Request headers to accept responses that
have exceeded their expiration time by no more than 600 seconds.</p>
<p>See also: RFC2616, 14.9.3</p>
<p>What is missing:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Pragma:</span> <span class="pre">no-cache</span></code> support <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1">https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Vary</span></code> header support <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6">https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6</a></p></li>
<li><p>Invalidation after updates or deletes <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10">https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10</a></p></li>
<li><p>… probably others ..</p></li>
</ul>
</dd></dl>

</section>
<section id="filesystem-storage-backend-default">
<span id="httpcache-storage-fs"></span><h4>Filesystem storage backend (default)<a class="headerlink" href="#filesystem-storage-backend-default" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.FilesystemCacheStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.httpcache.</span></span><span class="sig-name descname"><span class="pre">FilesystemCacheStorage</span></span><a class="reference internal" href="../_modules/scrapy/extensions/httpcache.html#FilesystemCacheStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.extensions.httpcache.FilesystemCacheStorage" title="Permalink to this definition">¶</a></dt>
<dd><p>File system storage backend is available for the HTTP cache middleware.</p>
<p>Each request/response pair is stored in a different directory containing
the following files:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">request_body</span></code> - the plain request body</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">request_headers</span></code> - the request headers (in raw HTTP format)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response_body</span></code> - the plain response body</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response_headers</span></code> - the request headers (in raw HTTP format)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">meta</span></code> - some metadata of this cache resource in Python <code class="docutils literal notranslate"><span class="pre">repr()</span></code>
format (grep-friendly format)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pickled_meta</span></code> - the same metadata in <code class="docutils literal notranslate"><span class="pre">meta</span></code> but pickled for more
efficient deserialization</p></li>
</ul>
<p>The directory name is made from the request fingerprint (see
<code class="docutils literal notranslate"><span class="pre">scrapy.utils.request.fingerprint</span></code>), and one level of subdirectories is
used to avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">cache</span><span class="o">/</span><span class="nb">dir</span><span class="o">/</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="mi">72</span><span class="o">/</span><span class="mi">72811</span><span class="n">f648e718090f041317756c03adb0ada46c7</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="dbm-storage-backend">
<span id="httpcache-storage-dbm"></span><h4>DBM storage backend<a class="headerlink" href="#dbm-storage-backend" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.DbmCacheStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.httpcache.</span></span><span class="sig-name descname"><span class="pre">DbmCacheStorage</span></span><a class="reference internal" href="../_modules/scrapy/extensions/httpcache.html#DbmCacheStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.extensions.httpcache.DbmCacheStorage" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Dbm">DBM</a> storage backend is also available for the HTTP cache middleware.</p>
<p>By default, it uses the <a class="reference external" href="https://docs.python.org/3/library/dbm.html#module-dbm" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dbm</span></code></a>, but you can change it with the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-HTTPCACHE_DBM_MODULE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_DBM_MODULE</span></code></a> setting.</p>
</dd></dl>

</section>
<section id="writing-your-own-storage-backend">
<span id="httpcache-storage-custom"></span><h4>Writing your own storage backend<a class="headerlink" href="#writing-your-own-storage-backend" title="Permalink to this heading">¶</a></h4>
<p>You can implement a cache storage backend by creating a Python class that
defines the methods described below.</p>
<span class="target" id="module-scrapy.extensions.httpcache"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.CacheStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.httpcache.</span></span><span class="sig-name descname"><span class="pre">CacheStorage</span></span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.CacheStorage.open_spider">
<span class="sig-name descname"><span class="pre">open_spider</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method gets called after a spider has been opened for crawling. It handles
the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="signals.html#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">open_spider</span></code></a> signal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has been opened</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.CacheStorage.close_spider">
<span class="sig-name descname"><span class="pre">close_spider</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method gets called after a spider has been closed. It handles
the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="signals.html#std-signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">close_spider</span></code></a> signal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has been closed</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.CacheStorage.retrieve_response">
<span class="sig-name descname"><span class="pre">retrieve_response</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.retrieve_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Return response if present in cache, or <code class="docutils literal notranslate"><span class="pre">None</span></code> otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which generated the request</p></li>
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request to find cached response for</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.CacheStorage.store_response">
<span class="sig-name descname"><span class="pre">store_response</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.store_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Store the given response in the cache.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which the response is intended</p></li>
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the corresponding request the spider generated</p></li>
<li><p><strong>response</strong> (<a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response to store in the cache</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p>In order to use your storage backend, set:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to the Python import path of your custom storage class.</p></li>
</ul>
</section>
<section id="httpcache-middleware-settings">
<h4>HTTPCache middleware settings<a class="headerlink" href="#httpcache-middleware-settings" title="Permalink to this heading">¶</a></h4>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCacheMiddleware</span></code> can be configured through the following
settings:</p>
<section id="httpcache-enabled">
<span id="std-setting-HTTPCACHE_ENABLED"></span><h5>HTTPCACHE_ENABLED<a class="headerlink" href="#httpcache-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether the HTTP cache will be enabled.</p>
</section>
<section id="httpcache-expiration-secs">
<span id="std-setting-HTTPCACHE_EXPIRATION_SECS"></span><h5>HTTPCACHE_EXPIRATION_SECS<a class="headerlink" href="#httpcache-expiration-secs" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Expiration time for cached requests, in seconds.</p>
<p>Cached requests older than this time will be re-downloaded. If zero, cached
requests will never expire.</p>
</section>
<section id="httpcache-dir">
<span id="std-setting-HTTPCACHE_DIR"></span><h5>HTTPCACHE_DIR<a class="headerlink" href="#httpcache-dir" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'httpcache'</span></code></p>
<p>The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP
cache will be disabled. If a relative path is given, is taken relative to the
project data dir. For more info see: <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="commands.html#topics-project-structure"><span class="std std-ref">Default structure of Scrapy projects</span></a>.</p>
</section>
<section id="httpcache-ignore-http-codes">
<span id="std-setting-HTTPCACHE_IGNORE_HTTP_CODES"></span><h5>HTTPCACHE_IGNORE_HTTP_CODES<a class="headerlink" href="#httpcache-ignore-http-codes" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>Don’t cache response with these HTTP codes.</p>
</section>
<section id="httpcache-ignore-missing">
<span id="std-setting-HTTPCACHE_IGNORE_MISSING"></span><h5>HTTPCACHE_IGNORE_MISSING<a class="headerlink" href="#httpcache-ignore-missing" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, requests not found in the cache will be ignored instead of downloaded.</p>
</section>
<section id="httpcache-ignore-schemes">
<span id="std-setting-HTTPCACHE_IGNORE_SCHEMES"></span><h5>HTTPCACHE_IGNORE_SCHEMES<a class="headerlink" href="#httpcache-ignore-schemes" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">['file']</span></code></p>
<p>Don’t cache responses with these URI schemes.</p>
</section>
<section id="httpcache-storage">
<span id="std-setting-HTTPCACHE_STORAGE"></span><h5>HTTPCACHE_STORAGE<a class="headerlink" href="#httpcache-storage" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></code></p>
<p>The class which implements the cache storage backend.</p>
</section>
<section id="httpcache-dbm-module">
<span id="std-setting-HTTPCACHE_DBM_MODULE"></span><h5>HTTPCACHE_DBM_MODULE<a class="headerlink" href="#httpcache-dbm-module" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'dbm'</span></code></p>
<p>The database module to use in the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#httpcache-storage-dbm"><span class="std std-ref">DBM storage backend</span></a>. This setting is specific to the DBM backend.</p>
</section>
<section id="httpcache-policy">
<span id="std-setting-HTTPCACHE_POLICY"></span><h5>HTTPCACHE_POLICY<a class="headerlink" href="#httpcache-policy" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.extensions.httpcache.DummyPolicy'</span></code></p>
<p>The class which implements the cache policy.</p>
</section>
<section id="httpcache-gzip">
<span id="std-setting-HTTPCACHE_GZIP"></span><h5>HTTPCACHE_GZIP<a class="headerlink" href="#httpcache-gzip" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, will compress all cached data with gzip.
This setting is specific to the Filesystem backend.</p>
</section>
<section id="httpcache-always-store">
<span id="std-setting-HTTPCACHE_ALWAYS_STORE"></span><h5>HTTPCACHE_ALWAYS_STORE<a class="headerlink" href="#httpcache-always-store" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, will cache pages unconditionally.</p>
<p>A spider may wish to have all responses available in the cache, for
future use with <code class="docutils literal notranslate"><span class="pre">Cache-Control:</span> <span class="pre">max-stale</span></code>, for instance. The
DummyPolicy caches all responses but never revalidates them, and
sometimes a more nuanced policy is desirable.</p>
<p>This setting still respects <code class="docutils literal notranslate"><span class="pre">Cache-Control:</span> <span class="pre">no-store</span></code> directives in responses.
If you don’t want that, filter <code class="docutils literal notranslate"><span class="pre">no-store</span></code> out of the Cache-Control headers in
responses you feed to the cache middleware.</p>
</section>
<section id="httpcache-ignore-response-cache-controls">
<span id="std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS"></span><h5>HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS<a class="headerlink" href="#httpcache-ignore-response-cache-controls" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>List of Cache-Control directives in responses to be ignored.</p>
<p>Sites often set “no-store”, “no-cache”, “must-revalidate”, etc., but get
upset at the traffic a spider can generate if it actually respects those
directives. This allows to selectively ignore Cache-Control directives
that are known to be unimportant for the sites being crawled.</p>
<p>We assume that the spider will not issue Cache-Control directives
in requests unless it actually needs them, so directives in requests are
not filtered.</p>
</section>
</section>
</section>
<section id="module-scrapy.downloadermiddlewares.httpcompression">
<span id="httpcompressionmiddleware"></span><h3>HttpCompressionMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpcompression" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.httpcompression.</span></span><span class="sig-name descname"><span class="pre">HttpCompressionMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/httpcompression.html#HttpCompressionMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.</p>
<p>This middleware also supports decoding <a class="reference external" href="https://www.ietf.org/rfc/rfc7932.txt">brotli-compressed</a> as well as
<a class="reference external" href="https://www.ietf.org/rfc/rfc8478.txt">zstd-compressed</a> responses, provided that <a class="reference external" href="https://pypi.org/project/Brotli/">brotli</a> or <a class="reference external" href="https://pypi.org/project/zstandard/">zstandard</a> is
installed, respectively.</p>
</dd></dl>

<section id="httpcompressionmiddleware-settings">
<h4>HttpCompressionMiddleware Settings<a class="headerlink" href="#httpcompressionmiddleware-settings" title="Permalink to this heading">¶</a></h4>
<section id="compression-enabled">
<span id="std-setting-COMPRESSION_ENABLED"></span><h5>COMPRESSION_ENABLED<a class="headerlink" href="#compression-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Compression middleware will be enabled.</p>
</section>
</section>
</section>
<section id="module-scrapy.downloadermiddlewares.httpproxy">
<span id="httpproxymiddleware"></span><h3>HttpProxyMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpproxy" title="Permalink to this heading">¶</a></h3>
<span class="target" id="std-reqmeta-proxy"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.httpproxy.</span></span><span class="sig-name descname"><span class="pre">HttpProxyMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/httpproxy.html#HttpProxyMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the HTTP proxy to use for requests, by setting the
<code class="docutils literal notranslate"><span class="pre">proxy</span></code> meta value for <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects.</p>
<p>Like the Python standard library module <a class="reference external" href="https://docs.python.org/3/library/urllib.request.html#module-urllib.request" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">urllib.request</span></code></a>, it obeys
the following environment variables:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">http_proxy</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">https_proxy</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">no_proxy</span></code></p></li>
</ul>
<p>You can also set the meta key <code class="docutils literal notranslate"><span class="pre">proxy</span></code> per-request, to a value like
<code class="docutils literal notranslate"><span class="pre">http://some_proxy_server:port</span></code> or <code class="docutils literal notranslate"><span class="pre">http://username:password@some_proxy_server:port</span></code>.
Keep in mind this value will take precedence over <code class="docutils literal notranslate"><span class="pre">http_proxy</span></code>/<code class="docutils literal notranslate"><span class="pre">https_proxy</span></code>
environment variables, and it will also ignore <code class="docutils literal notranslate"><span class="pre">no_proxy</span></code> environment variable.</p>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.offsite">
<span id="offsitemiddleware"></span><h3>OffsiteMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.offsite" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.offsite.OffsiteMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.offsite.</span></span><span class="sig-name descname"><span class="pre">OffsiteMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/offsite.html#OffsiteMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.offsite.OffsiteMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.11.2.</span></p>
</div>
<p>Filters out Requests for URLs outside the domains covered by the spider.</p>
<p>This middleware filters out every request whose host names aren’t in the
spider’s <a class="reference internal" href="spiders.html#scrapy.Spider.allowed_domains" title="scrapy.Spider.allowed_domains"><code class="xref py py-attr docutils literal notranslate"><span class="pre">allowed_domains</span></code></a> attribute.
All subdomains of any domain in the list are also allowed.
E.g. the rule <code class="docutils literal notranslate"><span class="pre">www.example.org</span></code> will also allow <code class="docutils literal notranslate"><span class="pre">bob.www.example.org</span></code>
but not <code class="docutils literal notranslate"><span class="pre">www2.example.com</span></code> nor <code class="docutils literal notranslate"><span class="pre">example.com</span></code>.</p>
<p>When your spider returns a request for a domain not belonging to those
covered by the spider, this middleware will log a debug message similar to
this one:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DEBUG</span><span class="p">:</span> <span class="n">Filtered</span> <span class="n">offsite</span> <span class="n">request</span> <span class="n">to</span> <span class="s1">'offsite.example'</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">offsite</span><span class="o">.</span><span class="n">example</span><span class="o">/</span><span class="n">some</span><span class="o">/</span><span class="n">page</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>To avoid filling the log with too much noise, it will only print one of
these messages for each new domain filtered. So, for example, if another
request for <code class="docutils literal notranslate"><span class="pre">offsite.example</span></code> is filtered, no log message will be
printed. But if a request for <code class="docutils literal notranslate"><span class="pre">other.example</span></code> is filtered, a message
will be printed (but only for the first request filtered).</p>
<p>If the spider doesn’t define an
<a class="reference internal" href="spiders.html#scrapy.Spider.allowed_domains" title="scrapy.Spider.allowed_domains"><code class="xref py py-attr docutils literal notranslate"><span class="pre">allowed_domains</span></code></a> attribute, or the
attribute is empty, the offsite middleware will allow all requests.</p>
<p>If the request has the <code class="xref py py-attr docutils literal notranslate"><span class="pre">dont_filter</span></code> attribute
set, the offsite middleware will allow the request even if its domain is not
listed in allowed domains.</p>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.redirect">
<span id="redirectmiddleware"></span><h3>RedirectMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.redirect" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.redirect.RedirectMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.redirect.</span></span><span class="sig-name descname"><span class="pre">RedirectMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/redirect.html#RedirectMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on response status.</p>
</dd></dl>

<p id="std-reqmeta-redirect_urls">The urls which the request goes through (while being redirected) can be found
in the <code class="docutils literal notranslate"><span class="pre">redirect_urls</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> key.</p>
<p id="std-reqmeta-redirect_reasons">The reason behind each redirect in <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_urls</span></code></a> can be found in the
<code class="docutils literal notranslate"><span class="pre">redirect_reasons</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> key. For
example: <code class="docutils literal notranslate"><span class="pre">[301,</span> <span class="pre">302,</span> <span class="pre">307,</span> <span class="pre">'meta</span> <span class="pre">refresh']</span></code>.</p>
<p>The format of a reason depends on the middleware that handled the corresponding
redirect. For example, <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code></a> indicates the triggering
response status code as an integer, while <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">MetaRefreshMiddleware</span></code></a>
always uses the <code class="docutils literal notranslate"><span class="pre">'meta</span> <span class="pre">refresh'</span></code> string as reason.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-REDIRECT_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_ENABLED</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_MAX_TIMES</span></code></a></p></li>
</ul>
<p id="std-reqmeta-dont_redirect">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> has <code class="docutils literal notranslate"><span class="pre">dont_redirect</span></code>
key set to True, the request will be ignored by this middleware.</p>
<p>If you want to handle some redirect status codes in your spider, you can
specify these in the <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> spider attribute.</p>
<p>For example, if you want the redirect middleware to ignore 301 and 302
responses (and pass them through to your spider) you can do this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">handle_httpstatus_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">301</span><span class="p">,</span> <span class="mi">302</span><span class="p">]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> key of <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key
<code class="docutils literal notranslate"><span class="pre">handle_httpstatus_all</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> if you want to allow any response code
for a request.</p>
<section id="redirectmiddleware-settings">
<h4>RedirectMiddleware settings<a class="headerlink" href="#redirectmiddleware-settings" title="Permalink to this heading">¶</a></h4>
<section id="redirect-enabled">
<span id="std-setting-REDIRECT_ENABLED"></span><h5>REDIRECT_ENABLED<a class="headerlink" href="#redirect-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Redirect middleware will be enabled.</p>
</section>
<section id="redirect-max-times">
<span id="std-setting-REDIRECT_MAX_TIMES"></span><h5>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">20</span></code></p>
<p>The maximum number of redirections that will be followed for a single request.
If maximum redirections are exceeded, the request is aborted and ignored.</p>
</section>
</section>
</section>
<section id="metarefreshmiddleware">
<h3>MetaRefreshMiddleware<a class="headerlink" href="#metarefreshmiddleware" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.redirect.</span></span><span class="sig-name descname"><span class="pre">MetaRefreshMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/redirect.html#MetaRefreshMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on meta-refresh html tag.</p>
</dd></dl>

<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">MetaRefreshMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-METAREFRESH_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_ENABLED</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-METAREFRESH_MAXDELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_MAXDELAY</span></code></a></p></li>
</ul>
<p>This middleware obey <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_MAX_TIMES</span></code></a> setting, <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-reqmeta-dont_redirect"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_redirect</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_urls</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-reqmeta-redirect_reasons"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_reasons</span></code></a> request meta keys as described
for <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code></a></p>
<section id="metarefreshmiddleware-settings">
<h4>MetaRefreshMiddleware settings<a class="headerlink" href="#metarefreshmiddleware-settings" title="Permalink to this heading">¶</a></h4>
<section id="metarefresh-enabled">
<span id="std-setting-METAREFRESH_ENABLED"></span><h5>METAREFRESH_ENABLED<a class="headerlink" href="#metarefresh-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Meta Refresh middleware will be enabled.</p>
</section>
<section id="metarefresh-ignore-tags">
<span id="std-setting-METAREFRESH_IGNORE_TAGS"></span><h5>METAREFRESH_IGNORE_TAGS<a class="headerlink" href="#metarefresh-ignore-tags" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>Meta tags within these tags are ignored.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.0: </span>The default value of <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a> changed from
<code class="docutils literal notranslate"><span class="pre">["script",</span> <span class="pre">"noscript"]</span></code> to <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.11.2: </span>The default value of <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a> changed from
<code class="docutils literal notranslate"><span class="pre">[]</span></code> to <code class="docutils literal notranslate"><span class="pre">["noscript"]</span></code>.</p>
</div>
</section>
<section id="metarefresh-maxdelay">
<span id="std-setting-METAREFRESH_MAXDELAY"></span><h5>METAREFRESH_MAXDELAY<a class="headerlink" href="#metarefresh-maxdelay" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">100</span></code></p>
<p>The maximum meta-refresh delay (in seconds) to follow the redirection.
Some sites use meta-refresh for redirecting to a session expired page, so we
restrict automatic redirection to the maximum delay.</p>
</section>
</section>
</section>
<section id="module-scrapy.downloadermiddlewares.retry">
<span id="retrymiddleware"></span><h3>RetryMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.retry" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.retry.RetryMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.retry.</span></span><span class="sig-name descname"><span class="pre">RetryMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/retry.html#RetryMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>A middleware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.</p>
</dd></dl>

<p>Failed pages are collected on the scraping process and rescheduled at the
end, once the spider has finished crawling all regular (non failed) pages.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-RETRY_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_ENABLED</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-RETRY_EXCEPTIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_EXCEPTIONS</span></code></a></p></li>
</ul>
<p id="std-reqmeta-dont_retry">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> has <code class="docutils literal notranslate"><span class="pre">dont_retry</span></code> key
set to True, the request will be ignored by this middleware.</p>
<p>To retry requests from a spider callback, you can use the
<a class="reference internal" href="#scrapy.downloadermiddlewares.retry.get_retry_request" title="scrapy.downloadermiddlewares.retry.get_retry_request"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_retry_request()</span></code></a> function:</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.retry.get_retry_request">
<span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.retry.</span></span><span class="sig-name descname"><span class="pre">get_retry_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request:</span> <span class="pre">Request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider:</span> <span class="pre">Spider</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reason:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">Exception</span> <span class="pre">|</span> <span class="pre">type[Exception]</span> <span class="pre">=</span> <span class="pre">'unspecified'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_retry_times:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority_adjust:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger:</span> <span class="pre">Logger</span> <span class="pre">=</span> <span class="pre">&lt;Logger</span> <span class="pre">scrapy.downloadermiddlewares.retry</span> <span class="pre">(WARNING)&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stats_base_key:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'retry'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/retry.html#get_retry_request"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.retry.get_retry_request" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object to retry the specified
request, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if retries of the specified request have been
exhausted.</p>
<p>For example, in a <a class="reference internal" href="spiders.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> callback, you could use it as
follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">:</span>
        <span class="n">new_request_or_none</span> <span class="o">=</span> <span class="n">get_retry_request</span><span class="p">(</span>
            <span class="n">response</span><span class="o">.</span><span class="n">request</span><span class="p">,</span>
            <span class="n">spider</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">reason</span><span class="o">=</span><span class="s1">'empty'</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">new_request_or_none</span>
</pre></div>
</div>
<p><em>spider</em> is the <a class="reference internal" href="spiders.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> instance which is asking for the
retry request. It is used to access the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#topics-settings"><span class="std std-ref">settings</span></a>
and <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="stats.html#topics-stats"><span class="std std-ref">stats</span></a>, and to provide extra logging context (see
<a class="reference external" href="https://docs.python.org/3/library/logging.html#logging.debug" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">logging.debug()</span></code></a>).</p>
<p><em>reason</em> is a string or an <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Exception</span></code></a> object that indicates the
reason why the request needs to be retried. It is used to name retry stats.</p>
<p><em>max_retry_times</em> is a number that determines the maximum number of times
that <em>request</em> can be retried. If not specified or <code class="docutils literal notranslate"><span class="pre">None</span></code>, the number is
read from the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="request-response.html#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key of the request. If the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="request-response.html#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key is not defined or <code class="docutils literal notranslate"><span class="pre">None</span></code>, the number
is read from the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a> setting.</p>
<p><em>priority_adjust</em> is a number that determines how the priority of the new
request changes in relation to <em>request</em>. If not specified, the number is
read from the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-RETRY_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_PRIORITY_ADJUST</span></code></a> setting.</p>
<p><em>logger</em> is the logging.Logger object to be used when logging messages</p>
<p><em>stats_base_key</em> is a string to be used as the base key for the
retry-related job stats</p>
</dd></dl>

<section id="retrymiddleware-settings">
<h4>RetryMiddleware Settings<a class="headerlink" href="#retrymiddleware-settings" title="Permalink to this heading">¶</a></h4>
<section id="retry-enabled">
<span id="std-setting-RETRY_ENABLED"></span><h5>RETRY_ENABLED<a class="headerlink" href="#retry-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Retry middleware will be enabled.</p>
</section>
<section id="retry-times">
<span id="std-setting-RETRY_TIMES"></span><h5>RETRY_TIMES<a class="headerlink" href="#retry-times" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">2</span></code></p>
<p>Maximum number of times to retry, in addition to the first download.</p>
<p>Maximum number of retries can also be specified per-request using
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="request-response.html#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> attribute of <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code>.
When initialized, the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="request-response.html#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key takes higher
precedence over the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a> setting.</p>
</section>
<section id="retry-http-codes">
<span id="std-setting-RETRY_HTTP_CODES"></span><h5>RETRY_HTTP_CODES<a class="headerlink" href="#retry-http-codes" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[500,</span> <span class="pre">502,</span> <span class="pre">503,</span> <span class="pre">504,</span> <span class="pre">522,</span> <span class="pre">524,</span> <span class="pre">408,</span> <span class="pre">429]</span></code></p>
<p>Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.</p>
<p>In some cases you may want to add 400 to <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a> because
it is a common code used to indicate server overload. It is not included by
default because HTTP specs say so.</p>
</section>
<section id="retry-exceptions">
<span id="std-setting-RETRY_EXCEPTIONS"></span><h5>RETRY_EXCEPTIONS<a class="headerlink" href="#retry-exceptions" title="Permalink to this heading">¶</a></h5>
<p>Default:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="s1">'twisted.internet.defer.TimeoutError'</span><span class="p">,</span>
    <span class="s1">'twisted.internet.error.TimeoutError'</span><span class="p">,</span>
    <span class="s1">'twisted.internet.error.DNSLookupError'</span><span class="p">,</span>
    <span class="s1">'twisted.internet.error.ConnectionRefusedError'</span><span class="p">,</span>
    <span class="s1">'twisted.internet.error.ConnectionDone'</span><span class="p">,</span>
    <span class="s1">'twisted.internet.error.ConnectError'</span><span class="p">,</span>
    <span class="s1">'twisted.internet.error.ConnectionLost'</span><span class="p">,</span>
    <span class="s1">'twisted.internet.error.TCPTimedOutError'</span><span class="p">,</span>
    <span class="s1">'twisted.web.client.ResponseFailed'</span><span class="p">,</span>
    <span class="ne">IOError</span><span class="p">,</span>
    <span class="s1">'scrapy.core.downloader.handlers.http11.TunnelError'</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>List of exceptions to retry.</p>
<p>Each list entry may be an exception type or its import path as a string.</p>
<p>An exception will not be caught when the exception type is not in
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-RETRY_EXCEPTIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_EXCEPTIONS</span></code></a> or when the maximum number of retries for a request
has been exceeded (see <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#std-setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a>). To learn about uncaught
exception propagation, see
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a>.</p>
</section>
<section id="retry-priority-adjust">
<span id="std-setting-RETRY_PRIORITY_ADJUST"></span><h5>RETRY_PRIORITY_ADJUST<a class="headerlink" href="#retry-priority-adjust" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">-1</span></code></p>
<p>Adjust retry request priority relative to original request:</p>
<ul class="simple">
<li><p>a positive priority adjust means higher priority.</p></li>
<li><p><strong>a negative priority adjust (default) means lower priority.</strong></p></li>
</ul>
</section>
</section>
</section>
<section id="module-scrapy.downloadermiddlewares.robotstxt">
<span id="robotstxtmiddleware"></span><span id="topics-dlmw-robots"></span><h3>RobotsTxtMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.robotstxt" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.robotstxt.</span></span><span class="sig-name descname"><span class="pre">RobotsTxtMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/robotstxt.html#RobotsTxtMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware filters out requests forbidden by the robots.txt exclusion
standard.</p>
<p>To make sure Scrapy respects robots.txt make sure the middleware is enabled
and the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_OBEY</span></code></a> setting is enabled.</p>
<p>The <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-ROBOTSTXT_USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_USER_AGENT</span></code></a> setting can be used to specify the
user agent string to use for matching in the <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> file. If it
is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the User-Agent header you are sending with the request or the
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">USER_AGENT</span></code></a> setting (in that order) will be used for determining
the user agent to use in the <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> file.</p>
<p>This middleware has to be combined with a <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parser.</p>
<p>Scrapy ships with support for the following <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parsers:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#protego-parser"><span class="std std-ref">Protego</span></a> (default)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#python-robotfileparser"><span class="std std-ref">RobotFileParser</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#rerp-parser"><span class="std std-ref">Robotexclusionrulesparser</span></a></p></li>
</ul>
<p>You can change the <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parser with the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-ROBOTSTXT_PARSER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_PARSER</span></code></a>
setting. Or you can also <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="#support-for-new-robots-parser"><span class="std std-ref">implement support for a new parser</span></a>.</p>
</dd></dl>

<p id="std-reqmeta-dont_obey_robotstxt">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> has
<code class="docutils literal notranslate"><span class="pre">dont_obey_robotstxt</span></code> key set to True
the request will be ignored by this middleware even if
<a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_OBEY</span></code></a> is enabled.</p>
<p>Parsers vary in several aspects:</p>
<ul class="simple">
<li><p>Language of implementation</p></li>
<li><p>Supported specification</p></li>
<li><p>Support for wildcard matching</p></li>
<li><p>Usage of <a class="reference external" href="https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#order-of-precedence-for-rules">length based rule</a>:
in particular for <code class="docutils literal notranslate"><span class="pre">Allow</span></code> and <code class="docutils literal notranslate"><span class="pre">Disallow</span></code> directives, where the most
specific rule based on the length of the path trumps the less specific
(shorter) rule</p></li>
</ul>
<p>Performance comparison of different parsers is available at <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3969">the following link</a>.</p>
<section id="protego-parser">
<span id="id1"></span><h4>Protego parser<a class="headerlink" href="#protego-parser" title="Permalink to this heading">¶</a></h4>
<p>Based on <a class="reference external" href="https://github.com/scrapy/protego">Protego</a>:</p>
<ul class="simple">
<li><p>implemented in Python</p></li>
<li><p>is compliant with <a class="reference external" href="https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt">Google’s Robots.txt Specification</a></p></li>
<li><p>supports wildcard matching</p></li>
<li><p>uses the length based rule</p></li>
</ul>
<p>Scrapy uses this parser by default.</p>
</section>
<section id="robotfileparser">
<span id="python-robotfileparser"></span><h4>RobotFileParser<a class="headerlink" href="#robotfileparser" title="Permalink to this heading">¶</a></h4>
<p>Based on <a class="reference external" href="https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobotFileParser</span></code></a>:</p>
<ul class="simple">
<li><p>is Python’s built-in <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parser</p></li>
<li><p>is compliant with <a class="reference external" href="https://www.robotstxt.org/norobots-rfc.txt">Martijn Koster’s 1996 draft specification</a></p></li>
<li><p>lacks support for wildcard matching</p></li>
<li><p>doesn’t use the length based rule</p></li>
</ul>
<p>It is faster than Protego and backward-compatible with versions of Scrapy before 1.8.0.</p>
<p>In order to use this parser, set:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-ROBOTSTXT_PARSER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_PARSER</span></code></a> to <code class="docutils literal notranslate"><span class="pre">scrapy.robotstxt.PythonRobotParser</span></code></p></li>
</ul>
</section>
<section id="robotexclusionrulesparser">
<span id="rerp-parser"></span><h4>Robotexclusionrulesparser<a class="headerlink" href="#robotexclusionrulesparser" title="Permalink to this heading">¶</a></h4>
<p>Based on <a class="reference external" href="https://pypi.org/project/robotexclusionrulesparser/">Robotexclusionrulesparser</a>:</p>
<ul class="simple">
<li><p>implemented in Python</p></li>
<li><p>is compliant with <a class="reference external" href="https://www.robotstxt.org/norobots-rfc.txt">Martijn Koster’s 1996 draft specification</a></p></li>
<li><p>supports wildcard matching</p></li>
<li><p>doesn’t use the length based rule</p></li>
</ul>
<p>In order to use this parser:</p>
<ul class="simple">
<li><p>Install <code class="docutils literal notranslate"><span class="pre">Robotexclusionrulesparser</span></code> by running
<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">robotexclusionrulesparser</span></code></p></li>
<li><p>Set <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-ROBOTSTXT_PARSER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_PARSER</span></code></a> setting to
<code class="docutils literal notranslate"><span class="pre">scrapy.robotstxt.RerpRobotParser</span></code></p></li>
</ul>
</section>
<section id="implementing-support-for-a-new-parser">
<span id="support-for-new-robots-parser"></span><h4>Implementing support for a new parser<a class="headerlink" href="#implementing-support-for-a-new-parser" title="Permalink to this heading">¶</a></h4>
<p>You can implement support for a new <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parser by subclassing
the abstract base class <a class="reference internal" href="#scrapy.robotstxt.RobotParser" title="scrapy.robotstxt.RobotParser"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobotParser</span></code></a> and
implementing the methods described below.</p>
<span class="target" id="module-scrapy.robotstxt"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.robotstxt.RobotParser">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.robotstxt.</span></span><span class="sig-name descname"><span class="pre">RobotParser</span></span><a class="reference internal" href="../_modules/scrapy/robotstxt.html#RobotParser"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.robotstxt.RobotParser" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="scrapy.robotstxt.RobotParser.allowed">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">allowed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_agent</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="reference internal" href="../_modules/scrapy/robotstxt.html#RobotParser.allowed"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.robotstxt.RobotParser.allowed" title="Permalink to this definition">¶</a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if  <code class="docutils literal notranslate"><span class="pre">user_agent</span></code> is allowed to crawl <code class="docutils literal notranslate"><span class="pre">url</span></code>, otherwise return <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>url</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a>) – Absolute URL</p></li>
<li><p><strong>user_agent</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a>) – User agent</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.robotstxt.RobotParser.from_crawler">
<em class="property"><span class="pre">abstract</span><span class="w"> </span><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="api.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">robotstxt_body</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="reference internal" href="../_modules/scrapy/robotstxt.html#RobotParser.from_crawler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.robotstxt.RobotParser.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse the content of a <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> file as bytes. This must be a class method.
It must return a new instance of the parser backend.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>crawler</strong> (<a class="reference internal" href="api.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance) – crawler which made the request</p></li>
<li><p><strong>robotstxt_body</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a>) – content of a <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> file.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="module-scrapy.downloadermiddlewares.stats">
<span id="downloaderstats"></span><h3>DownloaderStats<a class="headerlink" href="#module-scrapy.downloadermiddlewares.stats" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.stats.DownloaderStats">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.stats.</span></span><span class="sig-name descname"><span class="pre">DownloaderStats</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/stats.html#DownloaderStats"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.stats.DownloaderStats" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that stores stats of all requests, responses and exceptions that
pass through it.</p>
<p>To use this middleware you must enable the <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="settings.html#std-setting-DOWNLOADER_STATS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_STATS</span></code></a>
setting.</p>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.useragent">
<span id="useragentmiddleware"></span><h3>UserAgentMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.useragent" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.useragent.</span></span><span class="sig-name descname"><span class="pre">UserAgentMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/useragent.html#UserAgentMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that allows spiders to override the default user agent.</p>
<p>In order for a spider to override the default user agent, its <code class="docutils literal notranslate"><span class="pre">user_agent</span></code>
attribute must be set.</p>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.ajaxcrawl">
<span id="ajaxcrawlmiddleware"></span><span id="ajaxcrawl-middleware"></span><h3>AjaxCrawlMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.ajaxcrawl" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.ajaxcrawl.</span></span><span class="sig-name descname"><span class="pre">AjaxCrawlMiddleware</span></span><a class="reference internal" href="../_modules/scrapy/downloadermiddlewares/ajaxcrawl.html#AjaxCrawlMiddleware"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that finds ‘AJAX crawlable’ page variants based
on meta-fragment html tag.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Scrapy finds ‘AJAX crawlable’ pages for URLs like
<code class="docutils literal notranslate"><span class="pre">'http://example.com/!#foo=bar'</span></code> even without this middleware.
AjaxCrawlMiddleware is necessary when URL doesn’t contain <code class="docutils literal notranslate"><span class="pre">'!#'</span></code>.
This is often a case for ‘index’ or ‘main’ website pages.</p>
</div>
</dd></dl>

<section id="ajaxcrawlmiddleware-settings">
<h4>AjaxCrawlMiddleware Settings<a class="headerlink" href="#ajaxcrawlmiddleware-settings" title="Permalink to this heading">¶</a></h4>
<section id="ajaxcrawl-enabled">
<span id="std-setting-AJAXCRAWL_ENABLED"></span><h5>AJAXCRAWL_ENABLED<a class="headerlink" href="#ajaxcrawl-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether the AjaxCrawlMiddleware will be enabled. You may want to
enable it for <a class="hxr-hoverxref hxr-tooltip reference internal tooltipstered" href="broad-crawls.html#topics-broad-crawls"><span class="std std-ref">broad crawls</span></a>.</p>
</section>
</section>
<section id="httpproxymiddleware-settings">
<h4>HttpProxyMiddleware settings<a class="headerlink" href="#httpproxymiddleware-settings" title="Permalink to this heading">¶</a></h4>
<span class="target" id="std-setting-HTTPPROXY_ENABLED"></span><section id="httpproxy-enabled">
<span id="std-setting-HTTPPROXY_AUTH_ENCODING"></span><h5>HTTPPROXY_ENABLED<a class="headerlink" href="#httpproxy-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether or not to enable the <code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code>.</p>
</section>
<section id="httpproxy-auth-encoding">
<h5>HTTPPROXY_AUTH_ENCODING<a class="headerlink" href="#httpproxy-auth-encoding" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">"latin-1"</span></code></p>
<p>The default encoding for proxy authentication on <code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code>.</p>
</section>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="addons.html" class="btn btn-neutral float-left" title="Add-ons" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="spider-middleware.html" class="btn btn-neutral float-right" title="Spider Middleware" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr>

  <div role="contentinfo">
    <p>© Copyright Scrapy developers.
      <span class="lastupdated">Last updated on Nov 19, 2024.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 



  <div class="modal micromodal-slide rst-content" id="micromodal" aria-hidden="true">
    <div class="modal__overlay" tabindex="-1" data-micromodal-close="">
      <div class="modal__container" role="dialog" aria-modal="true" aria-labelledby="micromodal-title">
        <header class="modal__header">
          <h1 class="modal__title" id="micromodal-title"></h1>
          <button class="modal__close" aria-label="Close modal" data-micromodal-close=""></button>
        </header>
        <hr>
        <main class="modal__content" id="micromodal-content"></main>
        <footer class="modal__footer">
          <button class="modal__btn" data-micromodal-close="" aria-label="Close this dialog window">Close</button>
        </footer>
      </div>
    </div>
  </div>
</body></html>